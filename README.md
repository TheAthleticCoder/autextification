# CFP-AUTEXTIFICATION
**Link:** https://sites.google.com/view/autextification

By Harshit Gupta, Manav Chaudary and Aneesh Chavan - Team 30
---

# Overview

This repo contains the source code for our work on the Autextification shared task for our term project for SMAI. More information can be found at <a href="https://sites.google.com/view/autextification">https://sites.google.com/view/autextification</a>

## Description

The shared task itself has two subtasks
1. Task A: Distinguishing between human and AI generated text.
2. Task B: Distinguishing between text generated by a variety of text generation models.

Both subtasks have English and Spanish versions, with their associated train, dev and test datasets. 

Each Jupyter notebook in this repo contains code for each of our attempts. Running each method involves running each cell of code as instructed in each dataset.

## Env and model details

Conda env details can be found in requirements.txt

Model weights and checkpoints for models for each method can be found <a href="https://drive.google.com/drive/folders/1A2ts44xq1kJl5ReJJvJEYEXSzdftf6PB">here</a>

## Attempts made

We have tried the following methods:

### Task A
- BERT embeddings + PCA on varying embedding sizes
- BERT embeddings + linear layer
- BERT embeddings and sentiment embeddings + linear layer (spanish only)
- BERT embeddings and sentiment and POS embeddings + linear layer (spanish only)
- HuggingFaces pretrained sentence classification BERT

### Task B
- BERT embeddings and sentiment embeddings + linear layer
- BERT embeddings and sentiment and POS embeddings + linear layer
- HuggingFaces pretrained sentence classification BERT
- Random forest with xgboost + BERT embeddings
- Ensemble of BERTs treating each classification as binary classification
- Hierachical training of BERTs 


### Current Progress and Status:

Datasets Uploaded.
Models run.
Results compiled.
Results analyzed.
Report created.


(1) README.md  (containing instructions on how to run the project and some description of the codebase)
(2) conda/pip environment details (requirements.txt)
(3) Your trained checkpoints/models (include them using a cloud link if they are heavy, in the README.md)
(4) Your training, inference and preprocessing code
